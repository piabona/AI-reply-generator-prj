{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/swoo-nam/project_final_team1/blob/main/zeroshot_%EC%82%B0%EC%88%A0%ED%8F%89%EA%B7%A0.ipynb","timestamp":1695425965889}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"1tA4tqq9CKfR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695426027258,"user_tz":-540,"elapsed":27867,"user":{"displayName":"Yeseul Ryu","userId":"01398576250166115600"}},"outputId":"b076cd2c-8e64-4cc3-9393-086a6b7e255d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n","  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.17.2 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","source":["reviews_to_predict = ['맛','양','가격','서비스','배달']\n","\n"],"metadata":{"id":"a0IOYonkRRkX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch\n","from tqdm import tqdm\n","import pandas as pd\n","\n","# 첫 번째 모델\n","\n","# zeroshot_model30000 = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/프로젝트/final project/data/zero_model_epoch_2\").to('cuda')\n","model_name = \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\n","model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n","classifier = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer, device=0)\n","\n","\n","# 두 번째 모델\n","model_name = \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\n","model = AutoModelForSequenceClassification.from_pretrained(model_name).to('cuda')\n","classifier = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer, device=0)\n","\n","# 공통 토크나이저\n","tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n","\n","# 데이터 전처리\n","inputs = tokenizer(reviews_to_predict, truncation=True, padding=True, return_tensors=\"pt\").to('cuda')\n","dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'])\n","data_loader = DataLoader(dataset, batch_size=16, shuffle=False)\n","\n","# 모델 1 예측\n","scores_from_model1 = []\n","# classifier pipeline 미사용\n","with torch.no_grad():\n","    for batch in tqdm(data_loader):\n","        input_ids, attention_mask = (item.to('cuda') for item in batch)\n","        outputs = zeroshot_model30000(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        probs = torch.sigmoid(logits).cpu().numpy()\n","\n","        for prob in probs:\n","            scores = {candidate_labels[idx]: f\"{val:.2f}\" for idx, val in enumerate(prob)}\n","            scores_from_model1.append(scores)\n","\n","# 모델 2 예측\n","scores_from_model2 = []\n","# classifier pipeline 사용\n","for review in tqdm(reviews_to_predict):\n","    output = classifier(review, candidate_labels, multi_label=True)\n","    scores = {label: round(score, 2) for label, score in zip(output['labels'], output['scores'])}\n","    scores_from_model2.append(scores)\n","\n","averaged_scores_list = []\n","predicted_labels_list = []\n","\n","for score1, score2 in zip(scores_from_model1, scores_from_model2):\n","\n","    averaged_scores = {}\n","    predicted_labels = []\n","\n","    # 각 레이블에 대한 두 모델의 점수를 순회합니다.\n","    for label in score1.keys():\n","        # 두 모델의 점수를 산술평균\n","        averaged_score = (float(score1[label]) + float(score2[label])) / 2\n","        averaged_scores[label] = round(averaged_score, 2)\n","\n","        # 산술평균 점수가 0.5 이상이면 해당 레이블 추가\n","        if averaged_score > 0.5:\n","            predicted_labels.append(label)\n","\n","    # 각 리뷰에 대한 산술평균 점수를 averaged_scores에 추가.\n","    averaged_scores_list.append(\", \".join([f\"{k}: {v}\" for k, v in averaged_scores.items()]))\n","    # 각 리뷰에 대한 산술평균 점수가 0.5 이상인 레이블을 predicted_labels에 추가\n","    predicted_labels_list.append(\", \".join(predicted_labels))\n","\n","result_df = pd.DataFrame({\n","    'Review': reviews_to_predict,\n","    'Predicted_Labels': predicted_labels_list,\n","    'Averaged_Label_Scores': averaged_scores_list\n","})\n","\n","print(result_df)\n"],"metadata":{"id":"kF0AM29A_LdJ","colab":{"base_uri":"https://localhost:8080/","height":245},"executionInfo":{"status":"error","timestamp":1695426171106,"user_tz":-540,"elapsed":7659,"user":{"displayName":"Yeseul Ryu","userId":"01398576250166115600"}},"outputId":"b5753c36-daf2-4645-ed74-1bf2bb5ac19a"},"execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-7dbed582b47e>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# zeroshot_model30000 = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/프로젝트/final project/data/zero_model_epoch_2\").to('cuda')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"zero-shot-classification\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"7a7cpM_TRIVP"},"execution_count":null,"outputs":[]}]}